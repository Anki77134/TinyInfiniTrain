# TinyInfiniTrain ä½œä¸šå®ç°æŒ‡å—

## ç›®å½•
- [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
- [ä½œä¸šå®Œæˆé¡ºåº](#ä½œä¸šå®Œæˆé¡ºåº)
- [ä½œä¸šè¯¦è§£](#ä½œä¸šè¯¦è§£)
  - [ä½œä¸šäº”ï¼šDispatcher æœºåˆ¶ï¼ˆæ ¸å¿ƒåŸºç¡€ï¼‰](#ä½œä¸šäº”dispatcher-æœºåˆ¶æ ¸å¿ƒåŸºç¡€)
  - [ä½œä¸šä¸€ï¼šNeg ç®—å­å®ç°](#ä½œä¸šä¸€neg-ç®—å­å®ç°)
  - [ä½œä¸šäºŒï¼šçŸ©é˜µä¹˜æ³•å®ç°](#ä½œä¸šäºŒçŸ©é˜µä¹˜æ³•å®ç°)
  - [ä½œä¸šä¸‰ï¼šAdam ä¼˜åŒ–å™¨å®ç°](#ä½œä¸šä¸‰adam-ä¼˜åŒ–å™¨å®ç°)
  - [ä½œä¸šå››ï¼šTensor åŸºç¡€æ“ä½œ](#ä½œä¸šå››tensor-åŸºç¡€æ“ä½œ)
  - [ä½œä¸šå…­ï¼šGPT-2 ç«¯åˆ°ç«¯è®­ç»ƒ](#ä½œä¸šå…­gpt-2-ç«¯åˆ°ç«¯è®­ç»ƒ)

---

## é¡¹ç›®æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªå¤§æ¨¡å‹è®­ç»ƒç³»ç»Ÿçš„æ•™å­¦é¡¹ç›®ï¼Œé‡‡ç”¨äº†ç±»ä¼¼ PyTorch çš„è®¾è®¡æ¶æ„ï¼š
- **è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿ**ï¼šè‡ªåŠ¨è®¡ç®—æ¢¯åº¦çš„åå‘ä¼ æ’­æœºåˆ¶
- **å¤šè®¾å¤‡æ”¯æŒ**ï¼šCPU å’Œ CUDA ä¸¤ç§è®¾å¤‡
- **Kernel åˆ†å‘æœºåˆ¶**ï¼šæ ¹æ®è®¾å¤‡ç±»å‹è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„è®¡ç®—æ ¸
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šç®—å­ã€å¼ é‡ã€ä¼˜åŒ–å™¨ç­‰ç»„ä»¶è§£è€¦

**æ ¸å¿ƒæ¦‚å¿µï¼š**
- **Kernel**ï¼šå®é™…æ‰§è¡Œè®¡ç®—çš„å‡½æ•°ï¼ˆå¦‚çŸ©é˜µä¹˜æ³•ã€å–åç­‰ï¼‰
- **Dispatcher**ï¼šæ ¹æ®è®¾å¤‡ç±»å‹åˆ†å‘åˆ°å¯¹åº”çš„ Kernel
- **Autograd Function**ï¼šå®šä¹‰å‰å‘å’Œåå‘ä¼ æ’­é€»è¾‘
- **Tensor**ï¼šå¤šç»´æ•°ç»„ï¼Œæ”¯æŒè‡ªåŠ¨æ±‚å¯¼

---

## ä½œä¸šå®Œæˆé¡ºåº

**å»ºè®®æŒ‰ä»¥ä¸‹é¡ºåºå®Œæˆï¼ˆç”±åŸºç¡€åˆ°åº”ç”¨ï¼‰ï¼š**

1. âœ… **ä½œä¸šäº”** (Dispatcher) - æœ€åŸºç¡€ï¼Œå…¶ä»–ä½œä¸šéƒ½ä¾èµ–å®ƒ
2. âœ… **ä½œä¸šä¸€** (Neg ç®—å­) - æœ€ç®€å•ï¼Œç”¨æ¥éªŒè¯ Dispatcher æ˜¯å¦æ­£ç¡®
3. âœ… **ä½œä¸šäºŒ** (çŸ©é˜µä¹˜æ³•) - æ ¸å¿ƒè®¡ç®—ç®—å­
4. âœ… **ä½œä¸šä¸‰** (Adam ä¼˜åŒ–å™¨) - å‚æ•°æ›´æ–°
5. âœ… **ä½œä¸šå››** (Tensor æ“ä½œ) - å¼ é‡åŸºç¡€åŠŸèƒ½
6. âœ… **ä½œä¸šå…­** (GPT-2 è®­ç»ƒ) - ç«¯åˆ°ç«¯é›†æˆ

---

## ä½œä¸šè¯¦è§£

### ä½œä¸šäº”ï¼šDispatcher æœºåˆ¶ï¼ˆæ ¸å¿ƒåŸºç¡€ï¼‰

**éš¾åº¦ï¼š** â­â­â­
**åˆ†å€¼ï¼š** 20 åˆ†
**æ–‡ä»¶ï¼š** `infini_train/include/dispatcher.h`
**æµ‹è¯•ï¼š** `test/kernels/test_dispatcher.cc`

#### è¦å®ç°ä»€ä¹ˆï¼Ÿ

Dispatcher æ˜¯æ•´ä¸ªç³»ç»Ÿçš„æ ¸å¿ƒåˆ†å‘æœºåˆ¶ï¼Œè´Ÿè´£ï¼š
1. **æ³¨å†Œ Kernel**ï¼šå°†è®¾å¤‡ç±»å‹ã€å‡½æ•°åã€å‡½æ•°æŒ‡é’ˆç»‘å®šåˆ°ä¸€èµ·
2. **è°ƒç”¨ Kernel**ï¼šæ ¹æ®è®¾å¤‡ç±»å‹å’Œå‡½æ•°åæ‰¾åˆ°å¯¹åº”çš„å‡½æ•°å¹¶è°ƒç”¨
3. **è‡ªåŠ¨æ³¨å†Œ**ï¼šé€šè¿‡å®åœ¨ç¨‹åºå¯åŠ¨æ—¶è‡ªåŠ¨æ³¨å†Œæ‰€æœ‰ Kernel

#### éœ€è¦å®ç°çš„ 3 ä¸ªéƒ¨åˆ†

**1. KernelFunction::Call æ–¹æ³•ï¼ˆå‡½æ•°æŒ‡é’ˆè°ƒç”¨ï¼‰**

```cpp
template <typename RetT, class... ArgsT> RetT Call(ArgsT... args) const {
    // ä½ éœ€è¦åšçš„ï¼š
    // 1. å®šä¹‰å‡½æ•°æŒ‡é’ˆç±»å‹ï¼šRetT (*FuncT)(ArgsT...)
    // 2. å°† func_ptr_ è½¬æ¢ä¸º FuncT ç±»å‹
    // 3. è°ƒç”¨å‡½æ•°å¹¶è¿”å›ç»“æœ

    using FuncT = RetT (*)(ArgsT...);
    // TODO: å®ç°å‡½æ•°è°ƒç”¨é€»è¾‘
    // HINT: func_ptr_ æ˜¯ void* ç±»å‹ï¼Œéœ€è¦å…ˆè½¬æ¢æˆ FuncT
    //       ç„¶åè°ƒç”¨å®ƒï¼šresult = func(args...)
}
```

**å…³é”®ç‚¹ï¼š**
- `func_ptr_` æ˜¯ `void*`ï¼Œå­˜å‚¨äº†å‡½æ•°æŒ‡é’ˆ
- éœ€è¦å°†å®ƒè½¬æ¢ä¸º `RetT (*)(ArgsT...)` ç±»å‹
- ä½¿ç”¨ `reinterpret_cast` è¿›è¡Œç±»å‹è½¬æ¢
- è°ƒç”¨è½¬æ¢åçš„å‡½æ•°å¹¶è¿”å›ç»“æœ

**2. Dispatcher::Register æ–¹æ³•ï¼ˆæ³¨å†Œ Kernelï¼‰**

```cpp
template <typename FuncT> void Register(const KeyT &key, FuncT &&kernel) {
    // ä½ éœ€è¦åšçš„ï¼š
    // 1. æ£€æŸ¥è¿™ä¸ª key æ˜¯å¦å·²ç»æ³¨å†Œè¿‡ï¼ˆé˜²æ­¢é‡å¤æ³¨å†Œï¼‰
    // 2. å°† kernel åŒ…è£…æˆ KernelFunction å¯¹è±¡
    // 3. æ’å…¥åˆ° key_to_kernel_map_ ä¸­

    // TODO: å®ç° kernel æ³¨å†Œé€»è¾‘
    // HINT: KeyT æ˜¯ pair<DeviceType, string>
    //       å¦‚æœ key å·²å­˜åœ¨ï¼Œåº”è¯¥æŠ¥é”™ï¼ˆä½¿ç”¨ CHECKï¼‰
    //       ä½¿ç”¨ emplace æˆ– insert å°† {key, KernelFunction(kernel)} æ’å…¥ map
}
```

**å…³é”®ç‚¹ï¼š**
- `key` æ˜¯ `std::pair<DeviceType, std::string>`ï¼Œå¦‚ `{DeviceType::kCPU, "MatmulForward"}`
- æ£€æŸ¥æ˜¯å¦å·²æ³¨å†Œï¼š`CHECK(!key_to_kernel_map_.contains(key))`
- æ’å…¥ï¼š`key_to_kernel_map_.emplace(key, KernelFunction(std::forward<FuncT>(kernel)))`

**3. REGISTER_KERNEL å®ï¼ˆè‡ªåŠ¨æ³¨å†Œï¼‰**

```cpp
#define REGISTER_KERNEL(device, kernel_name, kernel_func) \
    // ä½ éœ€è¦åšçš„ï¼š
    // 1. åˆ›å»ºä¸€ä¸ªå…¨å±€é™æ€å¯¹è±¡ï¼Œåœ¨å…¶æ„é€ å‡½æ•°ä¸­æ³¨å†Œ kernel
    // 2. ä½¿ç”¨ç‰¹æ®Šçš„å‘½åé¿å…å†²çªï¼ˆå¦‚ï¼š__register_##kernel_name##deviceï¼‰

    // TODO: å®ç°è‡ªåŠ¨æ³¨å†Œå®
    // HINT: ä½¿ç”¨åŒ¿å namespace + static å˜é‡
    //       åœ¨å˜é‡åˆå§‹åŒ–æ—¶è°ƒç”¨ Dispatcher::Instance().Register()
```

**å…³é”®ç‚¹ï¼š**
- éœ€è¦åœ¨**å…¨å±€é™æ€åŒº**æ³¨å†Œï¼Œä¿è¯ç¨‹åºå¯åŠ¨æ—¶è‡ªåŠ¨æ‰§è¡Œ
- å¸¸è§å†™æ³•ï¼šåˆ›å»ºä¸€ä¸ªå…¨å±€é™æ€å˜é‡ï¼Œåœ¨æ„é€ æ—¶æ³¨å†Œ

**ç¤ºä¾‹æ¨¡å¼ï¼š**
```cpp
#define REGISTER_KERNEL(device, kernel_name, kernel_func) \
    namespace { \
        static int __register_##kernel_name##_##device = []() { \
            infini_train::Dispatcher::Instance().Register( \
                {infini_train::DeviceType::device, #kernel_name}, kernel_func); \
            return 0; \
        }(); \
    }
```

#### ç†è§£ Dispatcher å·¥ä½œæµç¨‹

```
1. ç¨‹åºå¯åŠ¨æ—¶ï¼š
   REGISTER_KERNEL(kCPU, NegForward, NegForwardå‡½æ•°)
   â†’ Dispatcher æ³¨å†Œï¼š{kCPU, "NegForward"} â†’ NegForwardå‡½æ•°æŒ‡é’ˆ

2. è¿è¡Œæ—¶è°ƒç”¨ï¼š
   auto kernel = Dispatcher::Instance().GetKernel({kCPU, "NegForward"});
   â†’ ä» map ä¸­æŸ¥æ‰¾å¯¹åº”çš„ KernelFunction

   auto result = kernel.Call<std::shared_ptr<Tensor>>(input);
   â†’ è°ƒç”¨å®é™…çš„ NegForward å‡½æ•°
```

#### å‚è€ƒä»£ç ä½ç½®

æŸ¥çœ‹å…¶ä»–æ–‡ä»¶å¦‚ä½•ä½¿ç”¨ Dispatcherï¼š
- `infini_train/src/autograd/elementwise.cc` ç¬¬ 31 è¡Œï¼ˆReciprocal::Forwardï¼‰
- `infini_train/src/kernels/cpu/elementwise.cc` ç¬¬ 267 è¡Œï¼ˆREGISTER_KERNEL ç”¨æ³•ï¼‰

---

### ä½œä¸šä¸€ï¼šNeg ç®—å­å®ç°

**éš¾åº¦ï¼š** â­
**åˆ†å€¼ï¼š** 5 åˆ†
**æ–‡ä»¶ï¼š** `infini_train/src/autograd/elementwise.cc`
**æµ‹è¯•ï¼š** `test/autograd/test_elementwise.cc`

#### è¦å®ç°ä»€ä¹ˆï¼Ÿ

å®ç° Negï¼ˆå–åï¼‰ç®—å­çš„å‰å‘å’Œåå‘ä¼ æ’­ï¼Œä¾‹å¦‚ï¼š
- å‰å‘ï¼š`output = -input`
- åå‘ï¼š`grad_input = -grad_output`

#### éœ€è¦å®ç°çš„ 2 ä¸ªå‡½æ•°

**1. Neg::Forwardï¼ˆå‰å‘ä¼ æ’­ï¼‰**

```cpp
std::vector<std::shared_ptr<Tensor>> Neg::Forward(
    const std::vector<std::shared_ptr<Tensor>> &input_tensors) {

    // ä½ éœ€è¦åšçš„ï¼š
    // 1. æ£€æŸ¥è¾“å…¥å¼ é‡æ•°é‡æ˜¯å¦ä¸º 1
    // 2. è·å–è¾“å…¥å¼ é‡çš„è®¾å¤‡ç±»å‹
    // 3. é€šè¿‡ Dispatcher è·å–å¯¹åº”è®¾å¤‡çš„ "NegForward" kernel
    // 4. è°ƒç”¨ kernel å¹¶è¿”å›ç»“æœ

    // TODO: å®ç°å‰å‘ä¼ æ’­
    // HINT: å‚è€ƒåŒæ–‡ä»¶ä¸­çš„ Reciprocal::Forwardï¼ˆç¬¬ 26-33 è¡Œï¼‰
}
```

**å‚è€ƒåŒæ–‡ä»¶ä¸­å·²å®ç°çš„ Reciprocal::Forwardï¼š**
```cpp
std::vector<std::shared_ptr<Tensor>> Reciprocal::Forward(...) {
    CHECK_EQ(input_tensors.size(), 1);  // 1. æ£€æŸ¥è¾“å…¥
    const auto &input = input_tensors[0];

    auto device = input->GetDevice().Type();  // 2. è·å–è®¾å¤‡ç±»å‹
    auto kernel = Dispatcher::Instance().GetKernel({device, "ReciprocalForward"});  // 3. è·å– kernel
    return {kernel.Call<std::shared_ptr<Tensor>>(input)};  // 4. è°ƒç”¨å¹¶è¿”å›
}
```

**2. Neg::Backwardï¼ˆåå‘ä¼ æ’­ï¼‰**

```cpp
std::vector<std::shared_ptr<Tensor>> Neg::Backward(
    const std::vector<std::shared_ptr<Tensor>> &grad_outputs) {

    // ä½ éœ€è¦åšçš„ï¼š
    // 1. æ£€æŸ¥æ¢¯åº¦è¾“å‡ºæ•°é‡æ˜¯å¦ä¸º 1
    // 2. è·å–æ¢¯åº¦è¾“å‡ºçš„è®¾å¤‡ç±»å‹
    // 3. é€šè¿‡ Dispatcher è·å– "NegBackward" kernel
    // 4. è°ƒç”¨ kernel è®¡ç®—è¾“å…¥æ¢¯åº¦

    // TODO: å®ç°åå‘ä¼ æ’­
    // HINT: å‚è€ƒ Reciprocal::Backwardï¼ˆç¬¬ 41-50 è¡Œï¼‰
}
```

#### å…³é”®ç‚¹

- **Kernel å·²å®ç°**ï¼š`infini_train/src/kernels/cpu/elementwise.cc` ç¬¬ 126-132 è¡Œ
  ```cpp
  std::shared_ptr<Tensor> NegForward(const std::shared_ptr<Tensor> &input) {
      return UnaryForward(input, [](float x) { return -x; });
  }

  std::shared_ptr<Tensor> NegBackward(const std::shared_ptr<Tensor> &grad_output) {
      return UnaryBackward(grad_output, nullptr, [](float) { return -1.0f; });
  }
  ```
- **å·²æ³¨å†Œ**ï¼šç¬¬ 267 è¡Œ `REGISTER_KERNEL(kCPU, NegForward, ...)`
- ä½ åªéœ€è¦åœ¨ autograd å±‚è°ƒç”¨å®ƒä»¬

---

### ä½œä¸šäºŒï¼šçŸ©é˜µä¹˜æ³•å®ç°

**éš¾åº¦ï¼š** â­â­
**åˆ†å€¼ï¼š** 15 åˆ†ï¼ˆCPU 5 åˆ† + CUDA 10 åˆ†ï¼‰
**æ–‡ä»¶ï¼š**
- CPU: `infini_train/src/kernels/cpu/linear.cc`
- CUDA: `infini_train/src/kernels/cuda/linear.cu`
**æµ‹è¯•ï¼š** `test/kernels/test_matmul.cc` å’Œ `test/kernels/test_matmul_cuda.cc`

#### è¦å®ç°ä»€ä¹ˆï¼Ÿ

çŸ©é˜µä¹˜æ³•æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€æ ¸å¿ƒçš„è®¡ç®—ï¼š
- å‰å‘ï¼š`C = A @ B`ï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰
- åå‘ï¼š
  - `grad_A = grad_C @ B^T`
  - `grad_B = A^T @ grad_C`

#### CPU ç‰ˆæœ¬å®ç°

**æ–‡ä»¶ï¼š** `infini_train/src/kernels/cpu/linear.cc`

**1. MatmulForwardï¼ˆå‰å‘è®¡ç®—ï¼‰**

```cpp
std::shared_ptr<Tensor> MatmulForward(
    const std::shared_ptr<Tensor> &input,
    const std::shared_ptr<Tensor> &other) {

    // ä½ éœ€è¦åšçš„ï¼š
    // 1. è·å–è¾“å…¥å¼ é‡çš„ç»´åº¦ä¿¡æ¯
    // 2. æ£€æŸ¥ç»´åº¦æ˜¯å¦å…¼å®¹ï¼ˆçŸ©é˜µä¹˜æ³•è§„åˆ™ï¼‰
    // 3. åˆ›å»ºè¾“å‡ºå¼ é‡
    // 4. ä½¿ç”¨ Eigen æ‰§è¡ŒçŸ©é˜µä¹˜æ³•

    // TODO: å®ç°çŸ©é˜µä¹˜æ³•
    // HINT: å‚è€ƒåŒæ–‡ä»¶ä¸­çš„ LinearForward å‡½æ•°ï¼ˆç¬¬ 36-77 è¡Œï¼‰
    //       input æ˜¯ [*, M, K]ï¼Œother æ˜¯ [*, K, N]
    //       output åº”è¯¥æ˜¯ [*, M, N]
    //       ä½¿ç”¨ EigenMatrix() è·å– Eigen çŸ©é˜µè§†å›¾
}
```

**å…³é”®æ­¥éª¤ï¼š**

```cpp
// 1. è·å–ç»´åº¦
const auto &input_dims = input->Dims();   // ä¾‹å¦‚ [2, 3, 4]
const auto &other_dims = other->Dims();   // ä¾‹å¦‚ [2, 4, 5]

// 2. æ£€æŸ¥å…¼å®¹æ€§
CHECK_GE(input_dims.size(), 2);
CHECK_GE(other_dims.size(), 2);
CHECK_EQ(input_dims.back(), other_dims[other_dims.size()-2]); // K å¿…é¡»ç›¸ç­‰

// 3. è®¡ç®—è¾“å‡ºç»´åº¦
auto output_dims = input_dims;
output_dims.back() = other_dims.back();  // [2, 3, 5]

// 4. åˆ›å»ºè¾“å‡º
auto output = std::make_shared<Tensor>(output_dims, DataType::kFLOAT32);

// 5. çŸ©é˜µä¹˜æ³•ï¼ˆä½¿ç”¨ Eigenï¼‰
output->EigenMatrix() = input->EigenMatrix() * other->EigenMatrix();
```

**2. MatmulBackwardï¼ˆåå‘ä¼ æ’­ï¼‰**

```cpp
std::tuple<std::shared_ptr<Tensor>, std::shared_ptr<Tensor>>
MatmulBackward(
    const std::shared_ptr<Tensor> &input,
    const std::shared_ptr<Tensor> &other,
    const std::shared_ptr<Tensor> &grad_output) {

    // ä½ éœ€è¦åšçš„ï¼š
    // 1. åˆ›å»º grad_input å’Œ grad_other
    // 2. è®¡ç®—æ¢¯åº¦ï¼š
    //    grad_input = grad_output @ other^T
    //    grad_other = input^T @ grad_output

    // TODO: å®ç°åå‘ä¼ æ’­
    // HINT: å‚è€ƒ LinearBackwardï¼ˆç¬¬ 79-123 è¡Œï¼‰
    //       ä½¿ç”¨ .transpose() è¿›è¡ŒçŸ©é˜µè½¬ç½®
}
```

**æ¢¯åº¦è®¡ç®—å…¬å¼ï¼š**
```
å‰å‘ï¼šC = A @ B
åå‘ï¼š
  âˆ‚L/âˆ‚A = âˆ‚L/âˆ‚C @ B^T
  âˆ‚L/âˆ‚B = A^T @ âˆ‚L/âˆ‚C
```

#### CUDA ç‰ˆæœ¬å®ç°

**æ–‡ä»¶ï¼š** `infini_train/src/kernels/cuda/linear.cu`

CUDA ç‰ˆæœ¬çš„å®ç°å’Œ CPU ç‰ˆæœ¬ç±»ä¼¼ï¼Œä½†ä½¿ç”¨ cuBLAS åº“è¿›è¡ŒåŠ é€Ÿï¼š

```cpp
std::shared_ptr<Tensor> MatmulForward(
    const std::shared_ptr<Tensor> &input,
    const std::shared_ptr<Tensor> &other) {

    // ä½ éœ€è¦åšçš„ï¼š
    // 1. è·å–ç»´åº¦ä¿¡æ¯
    // 2. åˆ›å»ºè¾“å‡ºå¼ é‡ï¼ˆæ³¨æ„ä½¿ç”¨ CUDA deviceï¼‰
    // 3. ä½¿ç”¨ cublasSgemm æ‰§è¡ŒçŸ©é˜µä¹˜æ³•

    // TODO: å®ç° CUDA çŸ©é˜µä¹˜æ³•
    // HINT: å‚è€ƒåŒæ–‡ä»¶ä¸­çš„ LinearForward å®ç°
    //       cuBLAS çš„çŸ©é˜µæ˜¯åˆ—ä¸»åºï¼ˆcolumn-majorï¼‰
    //       å¯èƒ½éœ€è¦è°ƒæ•´å‚æ•°é¡ºåº
}
```

**cuBLAS è°ƒç”¨ç¤ºä¾‹ï¼š**
```cpp
cublasSgemm(
    handle,           // cuBLAS handle
    CUBLAS_OP_N,      // ç¬¬ä¸€ä¸ªçŸ©é˜µæ˜¯å¦è½¬ç½®
    CUBLAS_OP_N,      // ç¬¬äºŒä¸ªçŸ©é˜µæ˜¯å¦è½¬ç½®
    N, M, K,          // çŸ©é˜µç»´åº¦
    &alpha,           // ç¼©æ”¾å› å­ï¼ˆé€šå¸¸ä¸º 1.0ï¼‰
    B_ptr, N,         // ç¬¬ä¸€ä¸ªçŸ©é˜µæŒ‡é’ˆå’Œ leading dimension
    A_ptr, K,         // ç¬¬äºŒä¸ªçŸ©é˜µæŒ‡é’ˆå’Œ leading dimension
    &beta,            // è¾“å‡ºç¼©æ”¾å› å­ï¼ˆé€šå¸¸ä¸º 0.0ï¼‰
    C_ptr, N          // è¾“å‡ºçŸ©é˜µæŒ‡é’ˆå’Œ leading dimension
);
```

---

### ä½œä¸šä¸‰ï¼šAdam ä¼˜åŒ–å™¨å®ç°

**éš¾åº¦ï¼š** â­
**åˆ†å€¼ï¼š** 15 åˆ†ï¼ˆCPU 5 åˆ† + CUDA 10 åˆ†ï¼‰
**æ–‡ä»¶ï¼š**
- CPU: `infini_train/src/kernels/cpu/accumulate_grad.cc`
- CUDA: `infini_train/src/kernels/cuda/accumulate_grad.cu`
**æµ‹è¯•ï¼š** `test/optimizer/test_adam.cc` å’Œ `test/optimizer/test_adam_cuda.cc`

#### è¦å®ç°ä»€ä¹ˆï¼Ÿ

Adam æ˜¯æœ€å¸¸ç”¨çš„ä¼˜åŒ–å™¨ï¼Œç»“åˆäº†åŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡ï¼š

```
m_t = beta1 * m_{t-1} + (1 - beta1) * grad        # ä¸€é˜¶åŠ¨é‡
v_t = beta2 * v_{t-1} + (1 - beta2) * grad^2      # äºŒé˜¶åŠ¨é‡
m_hat = m_t / (1 - beta1^t)                       # åå·®ä¿®æ­£
v_hat = v_t / (1 - beta2^t)
param = param - learning_rate * m_hat / (sqrt(v_hat) + eps)
```

#### CPU ç‰ˆæœ¬å®ç°

**æ–‡ä»¶ï¼š** `infini_train/src/kernels/cpu/accumulate_grad.cc`

```cpp
void AdamAccumulateGrad(
    const std::shared_ptr<Tensor> &grad,      // æ¢¯åº¦
    const std::shared_ptr<Tensor> &param,     // å‚æ•°
    const std::shared_ptr<Tensor> &m,         // ä¸€é˜¶åŠ¨é‡
    const std::shared_ptr<Tensor> &v,         // äºŒé˜¶åŠ¨é‡
    float learning_rate,                       // å­¦ä¹ ç‡
    float beta1,                               // ä¸€é˜¶åŠ¨é‡è¡°å‡ï¼ˆé€šå¸¸ 0.9ï¼‰
    float beta2,                               // äºŒé˜¶åŠ¨é‡è¡°å‡ï¼ˆé€šå¸¸ 0.999ï¼‰
    float eps,                                 // æ•°å€¼ç¨³å®šé¡¹ï¼ˆé€šå¸¸ 1e-8ï¼‰
    int64_t t) {                              // å½“å‰è¿­ä»£æ¬¡æ•°

    // ä½ éœ€è¦åšçš„ï¼š
    // 1. éå†æ‰€æœ‰å…ƒç´ 
    // 2. å¯¹æ¯ä¸ªå…ƒç´ åº”ç”¨ Adam æ›´æ–°å…¬å¼

    // TODO: å®ç° Adam ä¼˜åŒ–å™¨
    // HINT:
    //   - æ‰€æœ‰å¼ é‡çš„å…ƒç´ æ•°é‡ç›¸åŒï¼šgrad->NumElements()
    //   - ä½¿ç”¨ static_cast<float*>(tensor->DataPtr())[idx] è®¿é—®å…ƒç´ 
    //   - éœ€è¦è¿›è¡Œåå·®ä¿®æ­£ï¼šbeta1^t å¯ä»¥ç”¨ std::pow(beta1, t) è®¡ç®—
}
```

**å®ç°æ­¥éª¤ï¼š**

```cpp
void AdamAccumulateGrad(...) {
    for (int64_t idx = 0; idx < grad->NumElements(); ++idx) {
        // 1. è·å–å„ä¸ªå€¼
        float g = static_cast<float*>(grad->DataPtr())[idx];
        float* p = &static_cast<float*>(param->DataPtr())[idx];
        float* m_ptr = &static_cast<float*>(m->DataPtr())[idx];
        float* v_ptr = &static_cast<float*>(v->DataPtr())[idx];

        // 2. æ›´æ–°åŠ¨é‡
        *m_ptr = beta1 * (*m_ptr) + (1 - beta1) * g;
        *v_ptr = beta2 * (*v_ptr) + (1 - beta2) * g * g;

        // 3. åå·®ä¿®æ­£
        float m_hat = (*m_ptr) / (1 - std::pow(beta1, t));
        float v_hat = (*v_ptr) / (1 - std::pow(beta2, t));

        // 4. æ›´æ–°å‚æ•°
        *p = *p - learning_rate * m_hat / (std::sqrt(v_hat) + eps);
    }
}
```

#### CUDA ç‰ˆæœ¬å®ç°

**æ–‡ä»¶ï¼š** `infini_train/src/kernels/cuda/accumulate_grad.cu`

CUDA ç‰ˆæœ¬éœ€è¦å†™ kernel å‡½æ•°ï¼š

```cpp
__global__ void AdamKernel(
    float* grad, float* param, float* m, float* v,
    float lr, float beta1, float beta2, float eps,
    float bias_correction1, float bias_correction2,
    int64_t n) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        // å®ç° Adam æ›´æ–°ï¼ˆå’Œ CPU ç‰ˆæœ¬ç›¸åŒçš„é€»è¾‘ï¼‰
    }
}

void AdamAccumulateGrad(...) {
    // 1. è®¡ç®—åå·®ä¿®æ­£å› å­
    float bias_correction1 = 1 - std::pow(beta1, t);
    float bias_correction2 = 1 - std::pow(beta2, t);

    // 2. é…ç½® kernel å‚æ•°
    int block_size = 256;
    int grid_size = (grad->NumElements() + block_size - 1) / block_size;

    // 3. å¯åŠ¨ kernel
    AdamKernel<<<grid_size, block_size>>>(
        static_cast<float*>(grad->DataPtr()),
        static_cast<float*>(param->DataPtr()),
        // ... å…¶ä»–å‚æ•°
    );
}
```

---

### ä½œä¸šå››ï¼šTensor åŸºç¡€æ“ä½œ

**éš¾åº¦ï¼š** â­
**åˆ†å€¼ï¼š** 10 åˆ†
**æ–‡ä»¶ï¼š** `infini_train/src/tensor.cc`
**æµ‹è¯•ï¼š** `test/tensor/test_tensor.cc`

#### 1. Tensor::Flattenï¼ˆæ‰å¹³åŒ–æ“ä½œï¼‰

**ä½ç½®ï¼š** `infini_train/src/tensor.cc` ç¬¬ 279 è¡Œ

```cpp
std::shared_ptr<Tensor> Tensor::Flatten(int64_t start, int64_t end) {
    // ä½ éœ€è¦åšçš„ï¼š
    // 1. å¤„ç†è´Ÿæ•°ç´¢å¼•ï¼ˆ-1 è¡¨ç¤ºæœ€åä¸€ç»´ï¼‰
    // 2. è®¡ç®—æ–°çš„ shape
    // 3. è°ƒç”¨ View è¿”å›æ–°çš„è§†å›¾

    // TODO: å®ç°æ‰å¹³åŒ–
    // HINT: å°† [start, end] èŒƒå›´å†…çš„ç»´åº¦åˆå¹¶æˆä¸€ä¸ªç»´åº¦
    //       ä¾‹å¦‚ï¼šshape=[2,3,4,5], Flatten(1,3) -> [2,60]
}
```

**å®ç°æ€è·¯ï¼š**

```cpp
std::shared_ptr<Tensor> Tensor::Flatten(int64_t start, int64_t end) {
    int ndim = dims_.size();

    // 1. å¤„ç†è´Ÿæ•°ç´¢å¼•
    if (start < 0) start += ndim;
    if (end < 0) end += ndim;

    // 2. è®¡ç®—åˆå¹¶åçš„ç»´åº¦å¤§å°
    int64_t flatten_dim = 1;
    for (int i = start; i <= end; ++i) {
        flatten_dim *= dims_[i];
    }

    // 3. æ„é€ æ–° shape
    std::vector<int64_t> new_dims;
    for (int i = 0; i < start; ++i) {
        new_dims.push_back(dims_[i]);
    }
    new_dims.push_back(flatten_dim);
    for (int i = end + 1; i < ndim; ++i) {
        new_dims.push_back(dims_[i]);
    }

    // 4. è¿”å›æ–°è§†å›¾
    return Contiguous()->View(new_dims);
}
```

**ç¤ºä¾‹ï¼š**
```
åŸå§‹ shape: [2, 3, 4, 5]
Flatten(1, 2) -> [2, 12, 5]  (åˆå¹¶ç»´åº¦ 1 å’Œ 2ï¼š3*4=12)
Flatten(0, -1) -> [120]       (åˆå¹¶æ‰€æœ‰ç»´åº¦ï¼š2*3*4*5=120)
```

#### 2. Tensor::Backwardï¼ˆåå‘ä¼ æ’­ï¼‰

**ä½ç½®ï¼š** `infini_train/src/tensor.cc` ç¬¬ 356 è¡Œ

```cpp
void Tensor::Backward(
    std::shared_ptr<Tensor> gradient,
    bool retain_graph,
    bool create_graph) const {

    // ä½ éœ€è¦åšçš„ï¼š
    // 1. å¦‚æœæ²¡æœ‰æä¾› gradientï¼Œåˆ›å»ºå…¨ 1 çš„æ¢¯åº¦
    // 2. ä»å½“å‰èŠ‚ç‚¹å¼€å§‹æ‹“æ‰‘æ’åº
    // 3. æŒ‰åå‘é¡ºåºè°ƒç”¨æ¯ä¸ªèŠ‚ç‚¹çš„ Backward
    // 4. ç´¯ç§¯å¶å­èŠ‚ç‚¹çš„æ¢¯åº¦

    // TODO: å®ç°è‡ªåŠ¨å¾®åˆ†
    // HINT:
    //   - ä½¿ç”¨ grad_fn_ è·å–è®¡ç®—å›¾èŠ‚ç‚¹
    //   - è°ƒç”¨ grad_fn_->Backward() è®¡ç®—æ¢¯åº¦
    //   - å°†æ¢¯åº¦ç´¯åŠ åˆ° input_tensors çš„ grad_ ä¸­
}
```

**å®ç°æ€è·¯ï¼ˆæ‹“æ‰‘æ’åº + åå‘ä¼ æ’­ï¼‰ï¼š**

```cpp
void Tensor::Backward(...) const {
    // 1. åˆå§‹åŒ–æ¢¯åº¦
    if (!gradient) {
        gradient = std::make_shared<Tensor>(dims_, dtype_, GetDevice());
        gradient->Fill<float>(1.0f);
    }

    // 2. æ‹“æ‰‘æ’åºï¼ˆæ·±åº¦ä¼˜å…ˆæœç´¢ï¼‰
    std::vector<std::shared_ptr<autograd::Function>> topo_order;
    std::unordered_set<autograd::Function*> visited;

    std::function<void(std::shared_ptr<autograd::Function>)> dfs;
    dfs = [&](auto node) {
        if (!node || visited.count(node.get())) return;
        visited.insert(node.get());

        for (auto& input : node->GetInputTensors()) {
            if (input->grad_fn_) {
                dfs(input->grad_fn_);
            }
        }
        topo_order.push_back(node);
    };

    if (grad_fn_) dfs(grad_fn_);

    // 3. åå‘ä¼ æ’­
    std::unordered_map<Tensor*, std::shared_ptr<Tensor>> grad_map;
    grad_map[const_cast<Tensor*>(this)] = gradient;

    for (auto it = topo_order.rbegin(); it != topo_order.rend(); ++it) {
        auto node = *it;
        auto outputs = node->GetOutputTensors();

        // æ”¶é›†è¾“å‡ºæ¢¯åº¦
        std::vector<std::shared_ptr<Tensor>> grad_outputs;
        for (auto& out : outputs) {
            grad_outputs.push_back(grad_map[out.get()]);
        }

        // è®¡ç®—è¾“å…¥æ¢¯åº¦
        auto grad_inputs = node->Backward(grad_outputs);

        // ç´¯åŠ åˆ°è¾“å…¥å¼ é‡
        auto inputs = node->GetInputTensors();
        for (size_t i = 0; i < inputs.size(); ++i) {
            if (inputs[i]->requires_grad_) {
                if (inputs[i]->IsLeaf()) {
                    // å¶å­èŠ‚ç‚¹ï¼šç´¯åŠ åˆ° grad_
                    inputs[i]->grad_->Add(grad_inputs[i]);
                } else {
                    // ä¸­é—´èŠ‚ç‚¹ï¼šç´¯åŠ åˆ° grad_map
                    if (grad_map.count(inputs[i].get())) {
                        grad_map[inputs[i].get()]->Add(grad_inputs[i]);
                    } else {
                        grad_map[inputs[i].get()] = grad_inputs[i];
                    }
                }
            }
        }
    }
}
```

---

### ä½œä¸šå…­ï¼šGPT-2 ç«¯åˆ°ç«¯è®­ç»ƒ

**éš¾åº¦ï¼š** â­â­â­â­
**åˆ†å€¼ï¼š** 35 åˆ†
**æ–‡ä»¶ï¼š**
- `example/common/tiny_shakespeare_dataset.cc`
- `example/common/tokenizer.cc`
**æµ‹è¯•ï¼š** `test/example/test_gpt2.cc`

è¿™éƒ¨åˆ†æ˜¯ç«¯åˆ°ç«¯çš„é›†æˆæµ‹è¯•ï¼Œéœ€è¦å®Œæˆå‰é¢æ‰€æœ‰ä½œä¸šåæ‰èƒ½é€šè¿‡ã€‚

#### 1. æ•°æ®é›†è¯»å–ï¼ˆtiny_shakespeare_dataset.ccï¼‰

**æ–‡ä»¶æ ¼å¼ï¼š**
```
|<------- HEADER (1024 bytes) ------->|<--- DATA --->|
| magic | version | num_toks | reserved | token data  |
|  4B   |   4B    |    4B    |  1012B   |   å˜é•¿      |
```

**éœ€è¦å®ç°ï¼šReadTinyShakespeareFile**

```cpp
TinyShakespeareFile ReadTinyShakespeareFile(
    const std::string &path,
    size_t sequence_length) {

    // ä½ éœ€è¦åšçš„ï¼š
    // 1. æ‰“å¼€æ–‡ä»¶
    // 2. è¯»å– headerï¼ˆ1024 å­—èŠ‚ï¼‰
    // 3. è§£æ magicã€versionã€num_tokens
    // 4. æ ¹æ® version ç¡®å®š token ç±»å‹ï¼ˆuint16/uint32ï¼‰
    // 5. è¯»å–æ‰€æœ‰ token æ•°æ®
    // 6. æ„é€  TinyShakespeareFile è¿”å›

    std::ifstream ifs(path, std::ios::binary);
    CHECK(ifs.is_open()) << "Failed to open file: " << path;

    // TODO: å®ç°æ–‡ä»¶è§£æ
    // HINT: ä½¿ç”¨ ReadSeveralBytesFromIfstream å’Œ BytesToType
}
```

**å®ç°æ­¥éª¤ï¼š**
```cpp
// 1. è¯»å– header
auto header = ReadSeveralBytesFromIfstream(1024, &ifs);

// 2. è§£æå­—æ®µ
int magic = BytesToType<int>(header, 0);
int version = BytesToType<int>(header, 4);
int num_tokens = BytesToType<int>(header, 8);

// 3. ç¡®å®šç±»å‹
auto type = kTypeMap.at(version);
size_t token_size = kTypeToSize.at(type);

// 4. è¯»å– token æ•°æ®
size_t data_size = num_tokens * token_size;
auto data = ReadSeveralBytesFromIfstream(data_size, &ifs);

// 5. åˆ›å»º Tensor
auto tensor = std::make_shared<Tensor>(...);
std::memcpy(tensor->DataPtr(), data.data(), data_size);

// 6. è¿”å›ç»“æœ
TinyShakespeareFile result;
result.tensor = tensor;
result.dims = {num_tokens / sequence_length, sequence_length};
return result;
```

#### 2. Tokenizer å®ç°ï¼ˆtokenizer.ccï¼‰

ç±»ä¼¼çš„æ–‡ä»¶æ ¼å¼å’Œè§£æé€»è¾‘ï¼Œéœ€è¦å®ç°ï¼š
1. **æ„é€ å‡½æ•°**ï¼šè¯»å–è¯è¡¨æ–‡ä»¶
2. **Decode**ï¼šå°† token_id è½¬æ¢ä¸ºæ–‡æœ¬
3. **GenerateText**ï¼šè‡ªå›å½’ç”Ÿæˆæ–‡æœ¬

**GenerateText å®ç°æ€è·¯ï¼š**
```cpp
void Tokenizer::GenerateText(...) {
    for (int t = prompt_len; t < text_length; t++) {
        // 1. è°ƒç”¨æ¨¡å‹å‰å‘ä¼ æ’­
        auto logits = model.Forward(input)->Slice(-1, t, t+1);

        // 2. Softmax å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ
        auto probs = logits->Softmax(-1);

        // 3. é‡‡æ ·ä¸‹ä¸€ä¸ª token
        uint32_t next_token = sample(probs);

        // 4. è§£ç å¹¶è¾“å‡º
        std::cout << Decode(next_token);

        // 5. æ›´æ–°è¾“å…¥
        input[..., t+1] = next_token;
    }
}
```

---

## æµ‹è¯•å’ŒéªŒè¯

### è¿è¡Œæµ‹è¯•

```bash
# æ„å»ºé¡¹ç›®
make build

# è¿è¡Œæ‰€æœ‰æµ‹è¯•
make test-cpp

# è¿è¡Œå•ä¸ªæµ‹è¯•
cd build/Release
./test_dispatcher
./test_elementwise
./test_matmul
# ... ç­‰ç­‰
```

### æµ‹è¯•é€šè¿‡æ ‡å‡†

æ¯ä¸ªä½œä¸šå¯¹åº”çš„æµ‹è¯•ç”¨ä¾‹å…¨éƒ¨é€šè¿‡å³ä¸ºå®Œæˆã€‚

---

## è°ƒè¯•æŠ€å·§

1. **ä½¿ç”¨ LOG è¾“å‡º**ï¼š
   ```cpp
   LOG(INFO) << "Matrix shape: " << dims[0] << "x" << dims[1];
   LOG(ERROR) << "Unexpected error!";
   ```

2. **ä½¿ç”¨ CHECK æ–­è¨€**ï¼š
   ```cpp
   CHECK_EQ(a, b) << "a should equal b";
   CHECK_GT(size, 0) << "size must be positive";
   ```

3. **æŸ¥çœ‹ core dump**ï¼š
   ```bash
   ulimit -c unlimited  # å¼€å¯ core dump
   gdb ./test_xxx core  # è°ƒè¯•å´©æºƒ
   ```

4. **å•æ­¥è°ƒè¯•**ï¼š
   ```bash
   gdb ./test_dispatcher
   (gdb) break test_dispatcher.cc:33
   (gdb) run
   (gdb) next
   (gdb) print val
   ```

---

## å‚è€ƒèµ„æ–™

### å…³é”®æ¦‚å¿µ

- **Dispatcher æ¨¡å¼**ï¼šè®¾è®¡æ¨¡å¼ï¼Œç”¨äºæ ¹æ®æ¡ä»¶åˆ†å‘åˆ°ä¸åŒå®ç°
- **è‡ªåŠ¨å¾®åˆ†**ï¼šé€šè¿‡è®¡ç®—å›¾è‡ªåŠ¨è®¡ç®—æ¢¯åº¦
- **Kernel**ï¼šåº•å±‚è®¡ç®—å‡½æ•°ï¼Œé€šå¸¸é’ˆå¯¹ç‰¹å®šç¡¬ä»¶ä¼˜åŒ–

### ä»£ç å‚è€ƒ

å»ºè®®å…ˆé˜…è¯»è¿™äº›å·²å®ç°çš„ä»£ç ä½œä¸ºå‚è€ƒï¼š
- `infini_train/src/autograd/elementwise.cc` - å…¶ä»–ç®—å­çš„å®ç°
- `infini_train/src/kernels/cpu/elementwise.cc` - CPU kernel å®ç°
- `infini_train/src/kernels/cuda/elementwise.cu` - CUDA kernel å®ç°

### ç›¸å…³æ–‡æ¡£

- Eigen æ–‡æ¡£ï¼šhttps://eigen.tuxfamily.org/
- cuBLAS æ–‡æ¡£ï¼šhttps://docs.nvidia.com/cuda/cublas/
- Adam è®ºæ–‡ï¼šhttps://arxiv.org/abs/1412.6980

---

## å¸¸è§é—®é¢˜

### Q1: Dispatcher çš„ Call æ–¹æ³•æŠ¥é”™ "segmentation fault"
A: æ£€æŸ¥å‡½æ•°æŒ‡é’ˆè½¬æ¢æ˜¯å¦æ­£ç¡®ï¼Œç¡®ä¿è¿”å›ç±»å‹å’Œå‚æ•°ç±»å‹åŒ¹é…ã€‚

### Q2: æµ‹è¯•æ˜¾ç¤º "Kernel not found"
A: æ£€æŸ¥ REGISTER_KERNEL å®æ˜¯å¦æ­£ç¡®å®ç°ï¼Œæ˜¯å¦åœ¨å…¨å±€é™æ€åŒºæ‰§è¡Œã€‚

### Q3: çŸ©é˜µä¹˜æ³•ç»´åº¦ä¸åŒ¹é…
A: ä»”ç»†æ£€æŸ¥ input å’Œ other çš„ç»´åº¦ï¼Œç¡®ä¿æ»¡è¶³ `[M, K] @ [K, N] = [M, N]`ã€‚

### Q4: CUDA æµ‹è¯•å¤±è´¥
A: ç¡®ä¿ CUDA ç¯å¢ƒæ­£ç¡®é…ç½®ï¼Œæ£€æŸ¥ cuBLAS è°ƒç”¨å‚æ•°æ˜¯å¦æ­£ç¡®ï¼ˆæ³¨æ„åˆ—ä¸»åºï¼‰ã€‚

### Q5: Adam ä¼˜åŒ–å™¨æ”¶æ•›æ…¢æˆ–ä¸æ”¶æ•›
A: æ£€æŸ¥æ˜¯å¦è¿›è¡Œäº†åå·®ä¿®æ­£ï¼ˆbias correctionï¼‰ï¼Œæ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦åˆç†ã€‚

---

## æ€»ç»“

å®Œæˆè¿™äº›ä½œä¸šåï¼Œä½ å°†ç†è§£ï¼š
1. **è®¡ç®—å›¾å’Œè‡ªåŠ¨å¾®åˆ†**çš„å®ç°åŸç†
2. **å¤šè®¾å¤‡åˆ†å‘æœºåˆ¶**çš„è®¾è®¡æ¨¡å¼
3. **çŸ©é˜µè¿ç®—**çš„ CPU å’Œ GPU å®ç°
4. **ä¼˜åŒ–å™¨**çš„æ•°å­¦åŸç†å’Œå·¥ç¨‹å®ç°
5. **ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ ç³»ç»Ÿ**çš„æ¶æ„è®¾è®¡

ç¥ä½ é¡ºåˆ©å®Œæˆä½œä¸šï¼ğŸ’ª
